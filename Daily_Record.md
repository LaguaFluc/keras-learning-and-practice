# 2021.11.15总结

Done：

- [ ] 程序正确运行，跑完200个epochs
- [ ] 写完了关于数据输出的修正代码（需要后序的验证
- [ ] 发现了数据中存在的规律，模型大同小异，现在的建模需要往**数据处理**上走
- [ ] 发现：1可以尝试使用故障检测的方法来对数据进行预测，2尝试在进行故障检测之后对故障发生时刻进行预测



TODO:

- [ ] 验证，使用后序的修正代码
- [ ] 阅读3个kaggle中使用LSTM预测的代码
- [ ] 找到3个PHM期刊中的LSTM预测数据的，数据处理方法
- [ ] 对数据可视化之后的特征进行处理：考虑进行特征组合，特征筛选来处理特征
- [ ] 不使variancethreshold来筛选低方差变量，因为阈值设置差异

# 2021.11.16 总结

过拟合：[深度学习知识点总结四（防止过拟合的方法） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/97326991)

寿命预测比赛top分享：[科大讯飞2019机械核心部件寿命预测，亚军方案_datayx的文章-CSDN博客](https://blog.csdn.net/demm868/article/details/103171227)



遇到的问题：

1. Accuracy不高，0.0038，20类，1/20 = 0.05比猜还差
2. 特征都是连续型变量，且呈现出大致的指数分布，应该怎么进行特征工程处理数据
3. 数据效果不好，猜测可能是数据的问题，关于如何使用模型处理数据，有待考究



针对1：

1. 模型没有拟合，没有收敛，发现模型中有dropout层，这层本来是对于模型过拟合会产生一定的缓解作用，但是在这里没有拟合的情况下，就不需要对模型进行过多的dropout操作。
2. 删除了一个LSTM层，增加了一个Dense层，将模型简单话，因为对于这个数据不需要过于复杂的模型



## 阅读笔记

### [「AI不惑境」学习率和batchsize如何影响模型的性能？_收敛 (sohu.com)](https://www.sohu.com/a/333348924_120233360)

**1 为什么说学习率和batchsize**
$$
w_{t+1} = w_t - \eta \cfrac{1}{n} \sum_{x \in \mathcal{B} } \triangledown l(x, w_t)
$$
n: batch_size, $\eta$是学习率

lr: 影响模型收敛状态，

batch_size: 影响模型泛化性能



**2 学习率如何影响模型性能？**

学习率满足条件：
$$
\begin{aligned}
\sum_{i=0}^{\infty} \epsilon_i = \infty \\
\sum_{i=1}^{\infty}\epsilon_i^2 < \infty
\end{aligned}
$$
第一个：不管初始状态距离最优状态多元，总是可以收敛，

第二个：约束学习速率随着训练进行有效地降低，保证收敛稳定性（事实上，各个自适应的学习算法本质上就是再不断调整各个时刻地学习率。

学习率：决定权重迭代地步长

对模型性能地影响体现在两个方面：

- 初始学习率的大小
- 学习率的变换方案

**2.1 初始学习率对模型性能的影响**

不同学习率大对模型loss的影响

不考虑差异，如何确定最佳的初始学习率？

简单搜索发：从小到达开始训练模型，记录损失的变化，得到曲线

经验：0.1, 0.01

学习率增加，模型达到欠拟合或者过拟合状态，带那个数据集上会更加明显

**2.2 学习率变换策略对模型性能的影响**

lr很少有不变的，

两种更改方式：

- 设规则lr变化
- 自适应学习率变换

**2.2.1 预设规则学习率变化法**

常用： fixed, step, exp, inv, multistep, poly, sigmoid

做实验得到结论：step, multistep方法的收敛效果最好

其次，而exp, poly

最次，inv, fixed

新方法：cyclical learning rate

设置上下界，让学习了在其中变化

优点：在模型迭代后期更有利于克服因为lr不够而无法跳出鞍点的情况

确定上下界的方法： LR range test,使用不同的学习率得到精度曲线，获得精度升高和下降的两个拐点，作为上下界

SGDR: cuclical learning rate 变化更加平缓的周期性变化方法

**2.2.2 自适应学习率变化法**

Adagrad, Adam

现象：原理上，改进的自适应学习率算法都比SGD算法更有利于性能的提升；

实际上：精细调优过的SGD算法可以取得更好的结果

**2.3 小结**

不考虑其它因素，学习率的大小和迭代方法本身是一个非常敏感的参数；

如果经验不足，考虑从adam系列的默认参数开始；经验丰富，尝试更多的试验配置



**3 Batchsize如何影响模型性能**

先言：对batchsize没有lr那么敏感，进一步提升，需要调节batchsize

**3.1 大的batchsize减少训练时间，提高稳定性**

一样epoch，大的batchsize需要的batch数目减少，可以减少训练时间，目前多篇论文在1h内训练完ImageNet数据集，另一方面，大的batch size梯度的计算更加稳定，因为模型训练曲线会更加平滑，在微调的时候，大的batchsize 可能会取得更好的结果

**3.2 大的batchsize泛化能力下降**

**在一定范围内**，增加batch size有卒于收敛的稳定性，但是随着batchsize的增加，模型的性能会下降



研究[6] 表明大的batchsize 收敛到sharp minimum， 小的batchsize 收敛到flat minimum, 

结论： 后者泛化能力更强

区别：变化的趋势，一个快，一个慢

原因：（主要）小的batchsize带来的噪声有助于逃离sharp minimum

hoffer[7] 表明，大的batchsize性能下降时因为时间不够长，本质上并不是batchsize的问题，同样epochs, 参数更新变少，需要更长的迭代次数

**3.3 小结**

batchsize在变得很多时，会降低模型的泛化能力，再次之下，模型的性能变换随batchsize通常没有lr敏感

**4 学习率和batchsize的关系**

增加batchsize为原来的N倍，

保证更新后权重相等：线性缩放，lr = lr * N

保证更新后权重方差相等：lr = lr * sqrt(N)

前者用的多

常见的调整策略来看，lr 与 batchsize同时增加

lr: 非常敏感，不能太大，否则模型不收敛

batchsize: 也有影响

研究[8]： 衰减学习率可以通过增加batchszie来实现类似效果，从SGD的权重更新式子可以看出两者时等价的，文中有验证

研究[9]：对于fixed lr, 存在最优的batchsize 能够最大化测试精度，精度与batchsize和lr以及训练集的大小正相关

两建议：

- if 增加lr, batch size 最好也跟着增加，这样收敛更加稳定
- 尽量使用大的lr, 因为很多研究表明更大的lr有利于提高泛化能力。如果真的要衰减，尝试其它方法：eg增加batchsize, **lr 对模型的收敛影响真的很大，慎重调整**

